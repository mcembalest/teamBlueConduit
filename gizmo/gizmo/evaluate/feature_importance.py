import re
from sklearn.utils.validation import check_is_fitted
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.inspection import permutation_importance


# This module serves to reveal the fitted hyperparameters of
# BlueConduit's 4 primary models.

# This is the entry point to the script and the primary switching function.
def extract_feature_importances_as_df(model):
    feat_importance_funcs = {
        "LogisticRegression": get_LR_feature_importances,
        "RandomForestClassifier": get_RF_feature_importances,
        "KNeighborsClassifier": get_KNN_feature_importances,
        "XGBClassifier": get_XGB_feature_importances
    }

    estimator_name = get_estimator_name(model)
    custom_feat_importances = feat_importance_funcs[estimator_name]

    output_df = feat_importance_funcs[estimator_name](model)

    return output_df


def extract_feature_importances_generator(model):
    importances_df = extract_feature_importances_as_df(model)

    importances_cols = [col for col in importances_df if
        is_importance_type_col(col)]

    importance_names = {col: get_importance_type_name(col) for
        col in importances_cols}

    for col_name, raw_name in importance_names.items():
        yield {
            #'importance_col_name': col_name,
            'importance_name': raw_name,
            #'importances': importances_df[col_name],
            'estimator': get_estimator_name(model),
            'importance_df': importances_df[
                ["feature_name", "category", col_name]]
        }


def get_estimator_name(model):
    estimator = str(model["estimator"])
    if re.match("LogisticRegression", estimator):
        return "LogisticRegression"
    elif re.match("RandomForestClassifier", estimator):
        return "RandomForestClassifier"
    elif re.match("KNeighborsClassifier", estimator):
        return "KNeighborsClassifier"
    elif re.match("XGBClassifier", estimator):
        return "XGBClassifier"
    else:
        raise BaseException("estimator '{}' not yet recognized.".format(estimator))


def get_numeric_feature_names(model):
    numeric_columns = (model.named_steps["preprocessor"]
          .transformer_list[0][1]
          .transformers[0][2]
        )

    return numeric_columns


def get_categorical_feature_names(model):
    categorical_columns = (model.named_steps["preprocessor"]
          .transformer_list[0][1]
          .transformers[1][2]
        )

    return categorical_columns


def get_feature_names(model):
    numeric_columns = get_numeric_feature_names(model)

    categorical_columns = get_categorical_feature_names(model)

    feature_names = numeric_columns + categorical_columns

    return feature_names


def make_importance_col_name(importance_type):
    return "{}{}".format('imp-', importance_type)


def is_importance_type_col(col_name):
    start_str = make_importance_col_name('')
    return col_name.startswith(start_str)


def get_importance_type_name(col_name):
    start_str = make_importance_col_name('')
    if not is_importance_type_col(col_name):
        raise BaseException(
        """Column '{}' does not appear to be an importance type
        because it does not begin with the substring '{}'.
        """.format(col_name, start_str))

    return col_name.lstrip(start_str)


def get_feature_categories_df(model, categorical_features=None):

    numeric_feature_names = get_numeric_feature_names(model)
    categorical_feature_names = get_categorical_feature_names(model)


    raw_categories_all = list(model.named_steps['preprocessor'].transformer_list[0][1].transformers_[1][1]\
        .named_steps['one_hot_encoder'].get_feature_names())

    cleaned_categories_all = []

    output_df = pd.DataFrame({'feature_name':[],
                              'category':[],
                              'feature_one_hot_code':[],
                              'xgb_code':[]
                             })

    # Add the numeric feature names.
    numeric_features_df_temp = pd.DataFrame({
            'feature_name': numeric_feature_names
        })

    output_df = pd.concat([output_df, numeric_features_df_temp], ignore_index=True)

    # extract category names from one-hot-encoder in correct order.
    for i, feature in enumerate(categorical_feature_names):

        # feature codes are x0, x1, ... xN, generated by OneHotEncoder for N categorical features.
        feature_code = "x{}".format(i)

        raw_categories_specific = [s for s in raw_categories_all if s.startswith(feature_code)]

        # For, say, [x0_cat1, x0_cat2, ...], we're extracting categories 'cat1', 'cat2', ...
        categories = []
        compiled_re = re.compile('{}_(.*)'.format(feature_code))
        for raw_category in raw_categories_specific:
            categories.append(compiled_re.match(raw_category).group(1))

        categorical_feature_df_temp = pd.DataFrame({
            'feature_name': [feature for x in categories],
            'category': categories,
            'feature_one_hot_code': [feature_code for x in categories]
        })

        output_df = pd.concat([output_df, categorical_feature_df_temp], ignore_index=True)

        output_df['xgb_code'] = [f"f{i}" for i in range(len(output_df))]

    return output_df


# Logistic regression.
def get_LR_feature_importances(model):
    feature_names = get_feature_names(model)
    estimator = model.named_steps['estimator']
    feature_categories_df = get_feature_categories_df(model)

    # LR specific
    importance_types = ['coef_']
    importance_data = {}
    for importance_type in importance_types:
        try:
            importance_data[importance_type] = model.named_steps['estimator'].coef_[0]
        except AttributeError:
            raise AttributeError("'.coef_' doesn't exist; make sure the model has been fit!")

    col_name = make_importance_col_name('coef_')
    feature_categories_df[col_name] = model.named_steps['estimator'].coef_[0]

    return feature_categories_df


# Random Forest
def get_RF_feature_importances(model):
    feature_names = get_feature_names(model)
    estimator = model.named_steps['estimator']
    feature_categories_df = get_feature_categories_df(model)

    # RF specific
    importance_types = ['feature_importances_']
    importance_data = {}
    for importance_type in importance_types:
        try:
            importance_data[importance_type] = getattr(model.named_steps['estimator'], importance_type)
        except AttributeError:
            raise AttributeError("{} doesn't exist; make sure the model has been fit!".format(importance_type))

    col_name = make_importance_col_name('feature_importances_')
    feature_categories_df[col_name] = getattr(model.named_steps['estimator'], 'feature_importances_')

    return feature_categories_df


# K-Nearest Neigbors.
# Since BC only works with lat-lon models for KNN,
# so no feature importances. We use KNN as a naive baseline.
def get_KNN_feature_importances(model):
    return None



def get_XGB_feature_importances(model):
    feature_names = get_feature_names(model)
    estimator = model.named_steps['estimator']
    feature_categories_df = get_feature_categories_df(model)

    features_df = pd.DataFrame({"feature_name": feature_names})
    features_df["xgb_feat_name"] = [f"f{i}" for i in range(len(feature_names))]

    features = {}

    for importance_type in ['gain','weight','cover', 'total_gain', 'total_cover']:
        xgb_feat_dict = model.named_steps['estimator'].get_booster().get_score(importance_type=importance_type)
        feature_categories_df = feature_categories_df.merge(
            pd.DataFrame({
                'xgb_code': xgb_feat_dict.keys(),
                make_importance_col_name(importance_type): xgb_feat_dict.values()
            }),
            on='xgb_code',
            how='left'
        )

    col_name = make_importance_col_name('feature_importances_')
    feature_categories_df[col_name] = model.named_steps['estimator'].feature_importances_
    return feature_categories_df


def run_sklearn_permutation_importance(X, y, model, n_repeats=20):
    features_in_model = get_feature_names(model)
    X = X[features_in_model]

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, random_state=0)

    model.fit(X_train, y_train)
    model.score(X_val, y_val)

    importances_df = pd.DataFrame({
        'feature_name': features_in_model,
        'category': np.nan,
    })

    r = permutation_importance(model, X_val, y_val,
                               n_repeats=30,
                               random_state=0)

    importances_df['importances_mean'] = r.importances_mean
    importances_df['importances_std'] = r.importances_std

    # Copied from the original sklearn website code.
    # for i in r.importances_mean.argsort()[::-1]:
    #     if r.importances_mean[i] - 2 * r.importances_std[i] > -1:
    #         print(f"{X.columns[i]:<8} "
    #               f"{r.importances_mean[i]:.3f}"
    #               f" +/- {r.importances_std[i]:.3f}")

    return importances_df
